install.packages("leaps")
knitr::opts_chunk$set(echo = TRUE)
library(leaps)
set.seed(1)
x <- matrix(rnorm(1000 * 20), 1000, 20)
b <- rnorm(20)
b[2]<-0
b[5]<-0
b[18]<-0
b[12]<-0
eps <- rnorm(1000)
y <- x %*% b + eps
train <- sample(seq(1000), 100, replace = FALSE)
test <- (!train)
x.train <- x[train, ]
x.test <- x[test, ]
y.train <- y[train]
y.test <- y[test]
data.train <- data.frame(y = y.train, x = x.train)
regfit.full <- regsubsets(y ~ ., data = data.train, nvmax = 20)
train.mat <- model.matrix(y ~ ., data = data.train, nvmax = 20)
val.errors <- rep(NA, 20)
for (i in 1:20) {
coefi <- coef(regfit.full, id = i)
pred <- train.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((pred - y.train)^2)
}
plot(val.errors, xlab = "Number of predictors", ylab = "Training MSE", pch = 19, type = "l")
data.train <- data.frame(y = y.train, x = x.train)
regfit.full <- regsubsets(y ~ ., data = data.train, nvmax = 20)
train.mat <- model.matrix(y ~ ., data = data.train, nvmax = 20)
val.errors <- rep(NA, 20)
for (i in 1:20) {
coefi <- coef(regfit.full, id = i)
pred <- train.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((pred - y.train)^2)
}
plot(val.errors, xlab = "Number of predictors", ylab = "Training MSE", pch = 19, type = "b")
remove(ls())
rm(ls = ls())
ls()
remove(ls())
remove(ls = ls())
remove(list = ls())
library(leaps)
set.seed(1)
n = 1000
p = 20
x <-
x <- matrix(rnorm(n * p), n, p)
b <- rnorm(p)
b[2]<-0
b[5]<-0
b[18]<-0
b[12]<-0
eps <- rnorm(n)
y <- x %*% b + eps
train <- sample(seq(1000), 100, replace = FALSE)
test <- (!train)
y.train = y[train, ]
y.test = y[-train, ]
x.train = x[train, ]
x.test = x[-train, ]
data.train <- data.frame(y = y.train, x = x.train)
regfit.full <- regsubsets(y ~ ., data = data.train, nvmax = 20)
train.mat <- model.matrix(y ~ ., data = data.train, nvmax = 20)
val.errors <- rep(NA, 20)
for (i in 1:20) {
coefi <- coef(regfit.full, id = i)
pred <- train.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((pred - y.train)^2)
}
plot(val.errors, xlab = "Number of predictors", ylab = "Training MSE", pch = 19, type = "b")
data.train <- data.frame(y = y.train, x = x.train)
regfit.full <- regsubsets(y ~ ., data = data.train, nvmax = p)
train.mat <- model.matrix(y ~ ., data = data.train, nvmax = p)
val.errors <- rep(NA, p)
for (i in 1:p) {
coefi <- coef(regfit.full, id = i)
pred <- train.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((pred - y.train)^2)
}
plot(val.errors, xlab = "Number of predictors", ylab = "Training MSE", pch = 19, type = "b")
val.errors = rep(NA, p)
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
pred = as.matrix(x.test[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) %in%
x_cols]
val.errors[i] = mean((y.test - pred)^2)
}
library(leaps)
regfit.full = regsubsets(y ~ ., data = data.frame(x = x.train, y = y.train),
nvmax = p)
val.errors = rep(NA, p)
x_cols = colnames(x, do.NULL = FALSE, prefix = "x.")
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
pred = as.matrix(x.train[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) %in%
x_cols]
val.errors[i] = mean((y.train - pred)^2)
}
plot(val.errors, xlab = "Number of Predictors" ,ylab = "Training MSE", pch = 19, type = "b")
val.errors = rep(NA, p)
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
pred = as.matrix(x.test[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) %in%
x_cols]
val.errors[i] = mean((y.test - pred)^2)
}
plot(val.errors, xlab = "Number of Predictors", ylab = "Test MSE", pch = 19, type = "b")
library(leaps)
regfit.full = regsubsets(y ~ ., data = data.frame(x = x.train, y = y.train),
nvmax = p)
val.errors = rep(NA, p)
x_cols = colnames(x, do.NULL = FALSE, prefix = "x.")
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
pred = as.matrix(x.train[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) %in%
x_cols]
val.errors[i] = mean((y.train - pred)^2)
}
plot(val.errors, xlab = "Number of Predictors" ,ylab = "Training MSE", pch = 19, type = "b")
which.min(err.full)
library(leaps)
regfit.full = regsubsets(y ~ ., data = data.frame(x = x.train, y = y.train),
nvmax = p)
val.errors = rep(NA, p)
x_cols = colnames(x, do.NULL = FALSE, prefix = "x.")
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
pred = as.matrix(x.train[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) %in%
x_cols]
val.errors[i] = mean((y.train - pred)^2)
}
plot(val.errors, xlab = "Number of Predictors" ,ylab = "Training MSE", pch = 19, type = "b")
val.errors = rep(NA, p)
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
pred = as.matrix(x.test[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) %in%
x_cols]
val.errors[i] = mean((y.test - pred)^2)
}
plot(val.errors, xlab = "Number of Predictors", ylab = "Test MSE", pch = 19, type = "b")
which.min(val.errors)
val.errors = rep(NA, p)
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
pred = as.matrix(x.test[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) %in%
x_cols]
val.errors[i] = mean((y.test - pred)^2)
}
plot(val.errors, xlab = "Number of Predictors", ylab = "Test MSE", col = "red", pch = 19, type = "b")
library(leaps)
regfit.full = regsubsets(y ~ ., data = data.frame(x = x.train, y = y.train),
nvmax = p)
val.errors = rep(NA, p)
x_cols = colnames(x, do.NULL = FALSE, prefix = "x.")
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
pred = as.matrix(x.train[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) %in%
x_cols]
val.errors[i] = mean((y.train - pred)^2)
}
plot(val.errors, xlab = "Number of Predictors" ,ylab = "Training MSE", col = "blue" , pch = 19, type = "b")
coef(regfit.full, which.min(val.errors))[-1]
names(b) = colnames
coef(regfit.full, which.min(val.errors))[-1]
val.errors = rep(NA, p)
A = rep(NA, p)
B = rep(NA, p)
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
a[i] = length(coefi) - 1
b[i] = sqrt(sum((B[x_cols %in% names(coefi)] - coefi[names(coefi) %in% x_cols])^2) +
sum(B[!(x_cols %in% names(coefi))])^2)
}
val.errors = rep(NA, p)
A = rep(NA, p)
B = rep(NA, p)
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
A[i] = length(coefi) - 1
B[i] = sqrt(sum((b[x_cols %in% names(coefi)] - coefi[names(coefi) %in% x_cols])^2) +
sum(b[!(x_cols %in% names(coefi))])^2)
}
plot(x = A, y = B, xlab = "number of coefficients", ylab = "error between estimated and true coefficients")
val.errors = rep(NA, p)
A = rep(NA, p)
B = rep(NA, p)
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
A[i] = length(coefi) - 1
B[i] = sqrt(sum((b[x_cols %in% names(coefi)] - coefi[names(coefi) %in% x_cols])^2) +
sum(b[!(x_cols %in% names(coefi))])^2)
}
plot(x = A, y = B, xlab = "number of coefficients", ylab = "error between estimated and true coefficients", col = "green")
val.errors = rep(NA, p)
A = rep(NA, p)
B = rep(NA, p)
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
A[i] = length(coefi) - 1
B[i] = sqrt(sum((b[x_cols %in% names(coefi)] - coefi[names(coefi) %in% x_cols])^2) +
sum(b[!(x_cols %in% names(coefi))])^2)
}
plot(x = A, y = B, xlab = "number of coefficients", ylab = "error between estimated and true coefficients", col = "purple")
val.errors = rep(NA, p)
A = rep(NA, p)
B = rep(NA, p)
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
A[i] = length(coefi) - 1
B[i] = sqrt(sum((b[x_cols %in% names(coefi)] - coefi[names(coefi) %in% x_cols])^2) +
sum(b[!(x_cols %in% names(coefi))])^2)
}
plot(x = A, y = B, xlab = "number of coefficients", ylab = "error between estimated and true coefficients", col = "purple")
which.min(B)
knitr::opts_chunk$set(echo = TRUE)
library(leaps)
set.seed(1)
p = 20
n = 1000
x = matrix(rnorm(n * p), n, p)
B = rnorm(p)
B[3] = 0
B[4] = 0
B[9] = 0
B[19] = 0
B[10] = 0
eps = rnorm(p)
y = x %*% B + eps
train = sample(seq(1000), 100, replace = FALSE)
y.train = y[train, ]
y.test = y[-train, ]
x.train = x[train, ]
x.test = x[-train, ]
library(leaps)
regfit.full = regsubsets(y ~ ., data = data.frame(x = x.train, y = y.train),
nvmax = p)
val.errors = rep(NA, p)
x_cols = colnames(x, do.NULL = FALSE, prefix = "x.")
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
pred = as.matrix(x.train[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) %in%
x_cols]
val.errors[i] = mean((y.train - pred)^2)
}
plot(val.errors, ylab = "Training MSE", pch = 19, type = "b", col = "blue")
val.errors = rep(NA, p)
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
pred = as.matrix(x.test[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) %in%
x_cols]
val.errors[i] = mean((y.test - pred)^2)
}
plot(val.errors, ylab = "Test MSE", pch = 19, type = "b", col = "red")
which.min(val.errors)
remove(list = ls())
library(leaps)
set.seed(1)
p = 20
n = 1000
x = matrix(rnorm(n * p), n, p)
B = rnorm(p)
B[3] = 0
B[4] = 0
B[9] = 0
B[19] = 0
B[10] = 0
eps = rnorm(p)
y = x %*% B + eps
train = sample(seq(1000), 100, replace = FALSE)
y.train = y[train, ]
y.test = y[-train, ]
x.train = x[train, ]
x.test = x[-train, ]
library(leaps)
regfit.full = regsubsets(y ~ ., data = data.frame(x = x.train, y = y.train),
nvmax = p)
val.errors = rep(NA, p)
x_cols = colnames(x, do.NULL = FALSE, prefix = "x.")
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
pred = as.matrix(x.train[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) %in%
x_cols]
val.errors[i] = mean((y.train - pred)^2)
}
plot(val.errors, ylab = "Training MSE", pch = 19, type = "b", col = "blue")
val.errors = rep(NA, p)
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
pred = as.matrix(x.test[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) %in%
x_cols]
val.errors[i] = mean((y.test - pred)^2)
}
plot(val.errors, ylab = "Test MSE", pch = 19, type = "b", col = "red")
which.min(val.errors)
coef(regfit.full, which.min(val.errors))[-1]
coef(regfit.full, id = 16)
coef(regfit.full, id = 18)
knitr::opts_chunk$set(echo = TRUE)
library(leaps)
set.seed(1)
n = 1000
p = 20
x <- matrix(rnorm(n * p), n, p)
b <- rnorm(p)
b[2]<-0
b[5]<-0
b[18]<-0
b[12]<-0
eps <- rnorm(n)
y <- x %*% b + eps
train <- sample(seq(1000), 100, replace = FALSE)
test <- (!train)
y.train = y[train, ]
y.test = y[-train, ]
x.train = x[train, ]
x.test = x[-train, ]
library(leaps)
regfit.full = regsubsets(y ~ ., data = data.frame(x = x.train, y = y.train),
nvmax = p)
val.errors = rep(NA, p)
x_cols = colnames(x, do.NULL = FALSE, prefix = "x.")
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
pred = as.matrix(x.train[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) %in%
x_cols]
val.errors[i] = mean((y.train - pred)^2)
}
plot(val.errors, xlab = "Number of Predictors" ,ylab = "Training MSE", col = "blue" , pch = 19, type = "b")
abline(v = which.min(val.errors), lty=2)
test.errors = rep(NA, p)
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
pred = as.matrix(x.test[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) %in%
x_cols]
test.errors[i] = mean((y.test - pred)^2)
}
plot(test.errors, xlab = "Number of Predictors", ylab = "Test MSE", col = "red", pch = 19, type = "b")
abline(v = which.min(test.errors), lty = 2)
which.min(test.errors)
#coef(regfit.full, which.min(val.errors))[-1]
names(b) <- colnames(x)[2:20]
True.b = b[b!=0]
True.b = c(`(Intercept)` = 0, True.b)
Fitted = coeff(regfit.full, id = 16)
#coef(regfit.full, which.min(val.errors))[-1]
names(b) <- colnames(x)[2:20]
True.b = b[b!=0]
True.b = c(`(Intercept)` = 0, True.b)
Fitted = coef(regfit.full, id = 16)
rbind(True.b, Fitted, Err = (True.b-Fitted))
val.errors <- rep(NA, 20)
x_cols = colnames(x, do.NULL = FALSE, prefix = "x.")
for (i in 1:20) {
coefi <- coef(regfit.full, id = i)
val.errors[i] <- sqrt(sum((b[x_cols %in% names(coefi)] - coefi[names(coefi) %in% x_cols])^2) + sum(b[!(x_cols %in% names(coefi))])^2)
}
plot(val.errors, xlab = "Number of Coefficients", ylab = "Mean Squared Error for estimate and true coefficients", pch = 19, type = "b",col="purple")
remove(list = ls())
