knitr::opts_chunk$set(echo = TRUE)
set.seed(123)
n <- 500
p <- 2
x1 <- runif(500) - 0.5
x2 <- runif(500) - 0.5
y <- 1*(x1^2-x2^2 > 0)
plot(x1,x2, col = ifelse(y, 'red', 'black'), xlab = 'X1', ylab = 'X2', main = "Initial Data")
plot(x1,x2, col = ifelse(y, 'red', 'blue'), xlab = 'X1', ylab = 'X2', main = "Initial Data")
require(glm)
log.fit <- glm(y ~ x1 + x2, family = binomial)
summary(log.fit)
df = data.frame(x1 = x1, x2 = x2, y = as.factor(y))
log.prob = predict(log.fit, df, type = "response")
log.pred = ifelse(log.prob>0.52, 1, 0)
df.neg = df[log.pred == 0, ]
df.pos = df[log.pred == 1, ]
plot(df.pos$x1, df.neg$x2, col = "blue", xlab = "X1", ylab = "X2", main = "Logistic")
df = data.frame(x1 = x1, x2 = x2, y = as.factor(y))
log.prob = predict(log.fit, df, type = "response")
log.pred = ifelse(log.prob>0.52, 1, 0)
df.neg = df[log.pred == 0, ]
df.pos = df[log.pred == 1, ]
plot(df.pos$x1, df.pos$x2, col = "blue", xlab = "X1", ylab = "X2", main = "Logistic")
points(df.neg$x1, df.pos$x2, col = "red")
df = data.frame(x1 = x1, x2 = x2, y = as.factor(y))
log.prob = predict(log.fit, df, type = "response")
log.pred = ifelse(log.prob>0.52, 1, 0)
df.neg = df[log.pred == 0, ]
df.pos = df[log.pred == 1, ]
plot(df.pos$x1, df.pos$x2, col = "blue", xlab = "X1", ylab = "X2", main = "Logistic")
points(df.neg$x1, df.neg$x2, col = "red")
df = data.frame(x1 = x1, x2 = x2, y = as.factor(y))
log.prob = predict(log.fit, df, type = "response")
log.pred = ifelse(log.prob>0.52, 1, 0)
df.neg = df[log.pred == 0, ]
df.pos = df[log.pred == 1, ]
# plot(df.pos$x1, df.pos$x2, col = "blue", xlab = "X1", ylab = "X2", main = "Logistic")
# points(df.neg$x1, df.neg$x2, col = "red")
plot(df.pos, df.neg)
df = data.frame(x1 = x1, x2 = x2, y = as.factor(y))
log.prob = predict(log.fit, df, type = "response")
log.pred = ifelse(log.prob>0.52, 1, 0)
df.neg = df[log.pred == 0, ]
df.pos = df[log.pred == 1, ]
# plot(df.pos$x1, df.pos$x2, col = "blue", xlab = "X1", ylab = "X2", main = "Logistic")
# points(df.neg$x1, df.neg$x2, col = "red")
plot(df.pos, df.neg, col = ifelse(y, 'red', 'blue'))
df = data.frame(x1 = x1, x2 = x2, y = as.factor(y))
log.prob = predict(log.fit, df, type = "response")
log.pred = ifelse(log.prob>0.52, 1, 0)
df.neg = df[log.pred == 0, ]
df.pos = df[log.pred == 1, ]
# plot(df.pos$x1, df.pos$x2, col = "blue", xlab = "X1", ylab = "X2", main = "Logistic")
# points(df.neg$x1, df.neg$x2, col = "red")
plot(df.pos, df.neg, col = ifelse(log.pred, 'red', 'blue'))
df = data.frame(x1 = x1, x2 = x2, y = as.factor(y))
log.prob = predict(log.fit, df, type = "response")
log.pred = ifelse(log.prob>0.52, 1, 0)
df.neg = df[log.pred == 0, ]
df.pos = df[log.pred == 1, ]
plot(df.pos$x1, df.pos$x2, col = "blue", xlab = "X1", ylab = "X2", main = "Logistic")
points(df.neg$x1, df.neg$x2, col = "red")
plot(x1,x2, col = ifelse(y, 'blue', 'red'), xlab = 'X1', ylab = 'X2', main = "Initial Data")
df = data.frame(x1 = x1, x2 = x2, y = as.factor(y))
log.prob = predict(log.fit, df, type = "response")
log.pred = ifelse(log.prob>0.52, 1, 0)
df.neg = df[log.pred == 0, ]
df.pos = df[log.pred == 1, ]
plot(df.pos$x1, df.pos$x2, col = "blue", xlab = "X1", ylab = "X2", main = "Logistic")
points(df.neg$x1, df.neg$x2, col = "red")
# plot(x1,x2, col = ifelse(y, 'blue', 'red'), xlab = 'X1', ylab = 'X2', main = "Initial Data")
plot(x1[y == 0], x2[y == 0], col = "red", xlab = "X1", ylab = "X2", pch = "+")
points(x1[y == 1], x2[y == 1], col = "blue", pch = 4)
plot(x1,x2, col = ifelse(y, 'blue', 'red'), xlab = 'X1', ylab = 'X2', main = "Initial Data")
# df = data.frame(x1 = x1, x2 = x2, y = as.factor(y))
# log.prob = predict(log.fit, df, type = "response")
# log.pred = ifelse(log.prob>0.52, 1, 0)
# df.neg = df[log.pred == 0, ]
# df.pos = df[log.pred == 1, ]
# plot(df.pos$x1, df.pos$x2, col = "blue", xlab = "X1", ylab = "X2", main = "Logistic")
# points(df.neg$x1, df.neg$x2, col = "red")
data = data.frame(x1 = x1, x2 = x2, y = y)
lm.prob = predict(lm.fit, data, type = "response")
# df = data.frame(x1 = x1, x2 = x2, y = as.factor(y))
# log.prob = predict(log.fit, df, type = "response")
# log.pred = ifelse(log.prob>0.52, 1, 0)
# df.neg = df[log.pred == 0, ]
# df.pos = df[log.pred == 1, ]
# plot(df.pos$x1, df.pos$x2, col = "blue", xlab = "X1", ylab = "X2", main = "Logistic")
# points(df.neg$x1, df.neg$x2, col = "red")
data = data.frame(x1 = x1, x2 = x2, y = y)
lm.prob = predict(log.fit, data, type = "response")
lm.pred = ifelse(lm.prob > 0.52, 1, 0)
data.pos = data[lm.pred == 1, ]
data.neg = data[lm.pred == 0, ]
plot(data.pos$x1, data.pos$x2, col = "blue", xlab = "X1", ylab = "X2", pch = "+")
points(data.neg$x1, data.neg$x2, col = "red", pch = 4)
df = data.frame(x1 = x1, x2 = x2, y = as.factor(y))
log.prob = predict(log.fit, df, type = "response")
log.pred = ifelse(log.prob>0.52, 1, 0)
df.neg = df[log.pred == 0, ]
df.pos = df[log.pred == 1, ]
plot(df.pos$x1, df.pos$x2, col = "blue", xlab = "X1", ylab = "X2", main = "Logistic")
points(df.neg$x1, df.neg$x2, col = "red")
log.fit2 = glm(y ~ x1 + x2 + I(x1^2) + I(x2^2) + I(x1*x2), data = df, family = "binomial")
summary(log.fit2)
# log.fit2 = glm(y ~ x1 + x2 + I(x1^2) + I(x2^2) + I(x1*x2), data = df, family = "binomial")
log.fit2 = glm(y ~ poly(x1,2) + poly(x2,2) + I(x1 * x2), data = df, family = "binomial")
summary(log.fit2)
log.fit2 = glm(y ~ x1 + x2 + I(x1^2) + I(x2^2) + I(x1*x2), data = df, family = "binomial")
# log.fit2 = glm(y ~ poly(x1,2) + poly(x2,2) + I(x1 * x2), data = df, family = "binomial")
summary(log.fit2)
log.prob1 = predict(log.fit2, df, type = "response")
log.pred1 = ifelse(log.prob1>0.52, 1, 0)
df.neg1 = df[log.pred1 == 0, ]
df.pos1 = df[log.pred1 == 1, ]
plot(df.pos1$x1, df.pos1$x2, col = "blue", xlab = "X1", ylab = "X2", main = "Logistic")
points(df.neg1$x1, df.neg1$x2, col = "red")
log.prob1 = predict(log.fit2, df, type = "response")
log.pred1 = ifelse(log.prob1>0.52, 1, 0)
df.neg1 = df[log.pred1 == 0, ]
df.pos1 = df[log.pred1 == 1, ]
plot(df.pos1$x1, df.pos1$x2, col = "blue", xlab = "X1", ylab = "X2")
points(df.neg1$x1, df.neg1$x2, col = "red")
library(e1071)
tune.out <- tune(svm, y ~ ., data = df, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100, 1000)))
print(summary(tune.out))
library(e1071)
tune.out <- tune(svm, y ~ ., data = df, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100, 1000)))
summary(tune.out)
library(e1071)
# tune.out <- tune(svm, y ~ ., data = df, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100, 1000)))
# summary(tune.out)
names(tune.out)
library(e1071)
tune.out <- tune(svm, y ~ ., data = df, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100, 1000)))
summary(tune.out)
names(tune.out)
bestmod = tune.out$best.model
svm.fit = svm(as.factor(y) ~ x1 + x2, data = df, kernel = "linear", cost = bestmod)
library(e1071)
tune.out <- tune(svm, y ~ ., data = df, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100, 1000)))
summary(tune.out)
names(tune.out)
bestmod = tune.out$best.model
svm.fit = svm(y ~ x1 + x2, data = df, kernel = "linear", cost = bestmod)
library(e1071)
tune.out <- tune(svm, y ~ ., data = df, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100, 1000)))
summary(tune.out)
names(tune.out)
bestmod = tune.out$best.model
svm.fit = svm(as.numeric(as.character(y)) ~ x1 + x2, data = df, kernel = "linear", cost = bestmod)
library(e1071)
# tune.out <- tune(svm, y ~ ., data = df, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100, 1000)))
# summary(tune.out)
# names(tune.out)
# bestmod = tune.out$best.model
svm.fit = svm(as.factor(y) ~ x1 + x2, data = df, kernel = "linear", cost = bestmod)
library(e1071)
# tune.out <- tune(svm, y ~ ., data = df, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100, 1000)))
# summary(tune.out)
# names(tune.out)
# bestmod = tune.out$best.model
svm.fit = svm(as.factor(y) ~ x1 + x2, data = df, kernel = "linear", cost = 0.1)
svm.pred = predict(svm.fit, df)
data.pos = data[svm.pred == 1, ]
data.neg = data[svm.pred == 0, ]
plot(data.pos$x1, data.pos$x2, col = "blue", xlab = "X1", ylab = "X2")
points(data.neg$x1, data.neg$x2, col = "red")
library(e1071)
tune.out <- tune(svm, y ~ ., data = df, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100, 1000)))
summary(tune.out)
names(tune.out)
bestmod = tune.out$best.model
y_hat <- predict(bestmod, newdata = data.frame(x1 = x1, x2 = x2))
y_hat <- as.numeric(as.character(y_hat))
plot(x1, x2, col = ifelse(y, "blue", "red"), xlab = "X1", ylab = "X2")
# svm.fit = svm(as.factor(y) ~ x1 + x2, data = df, kernel = "linear", cost = 0.1)
# svm.pred = predict(svm.fit, df)
# data.pos = data[svm.pred == 1, ]
# data.neg = data[svm.pred == 0, ]
# plot(data.pos$x1, data.pos$x2, col = "blue", xlab = "X1", ylab = "X2")
# points(data.neg$x1, data.neg$x2, col = "red")
library(e1071)
tune.out <- tune(svm, y ~ ., data = df, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100, 1000)))
summary(tune.out)
names(tune.out)
bestmod = tune.out$best.model
y_hat <- predict(bestmod, newdata = data.frame(x1 = x1, x2 = x2))
y_hat <- as.numeric(as.character(y_hat))
plot(x1, x2, col = ifelse(y_hat, "blue", "red"), xlab = "X1", ylab = "X2")
# svm.fit = svm(as.factor(y) ~ x1 + x2, data = df, kernel = "linear", cost = 0.1)
# svm.pred = predict(svm.fit, df)
# data.pos = data[svm.pred == 1, ]
# data.neg = data[svm.pred == 0, ]
# plot(data.pos$x1, data.pos$x2, col = "blue", xlab = "X1", ylab = "X2")
# points(data.neg$x1, data.neg$x2, col = "red")
tune.out1 <- tune(svm, y ~ ., data = dat, kernel = "radial", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100, 1000), gamma = c(0.5, 1, 2, 3, 4)))
tune.out1 <- tune(svm, y ~ ., data = df, kernel = "radial", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100, 1000), gamma = c(0.5, 1, 2, 3, 4)))
summary(tune.out1)
names(tune.out1)
bestmod1 = tune.out1$best.model
y_hat1 <- predict(bestmod1, newdata = data.frame(x1 = x1, x2 = x2))
y_hat1 <- as.numeric(as.character(y_hat1))
plot(x1, x2, col = ifelse(y_hat1, "blue", "red"), xlab = "X1", ylab = "X2")
library(e1071)
# tune.out <- tune(svm, y ~ ., data = df, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100, 1000)))
# summary(tune.out)
# names(tune.out)
# bestmod = tune.out$best.model
# y_hat <- predict(bestmod, newdata = data.frame(x1 = x1, x2 = x2))
# y_hat <- as.numeric(as.character(y_hat))
# plot(x1, x2, col = ifelse(y_hat, "blue", "red"), xlab = "X1", ylab = "X2")
svm.fit=svm(y~.,data=data.frame(x1,x2,y=as.factor(y)),kernel='linear')
svm.pred=predict(svm.fit,data.frame(x1,x2),type='response')
plot(x1,x2,col=ifelse(svm.pred!=0,'red','blue'),pch=ifelse(svm.pred == y,1,4))
tune.out1 <- tune(svm, y ~ ., data = df, kernel = "radial", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100, 1000), gamma = c(0.5, 1, 2, 3, 4)))
summary(tune.out1)
names(tune.out1)
bestmod1 = tune.out1$best.model
y_hat1 <- predict(bestmod1, newdata = data.frame(x1 = x1, x2 = x2))
y_hat1 <- as.numeric(as.character(y_hat1))
plot(x1, x2, col = ifelse(y_hat1, "blue", "red"), xlab = "X1", ylab = "X2")
library(e1071)
tune.out <- tune(svm, y ~ ., data = df, kernel = "linear",
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100, 1000)))
summary(tune.out)
names(tune.out)
bestmod = tune.out$best.model
y_hat <- predict(bestmod, newdata = data.frame(x1 = x1, x2 = x2))
y_hat <- as.numeric(as.character(y_hat))
plot(x1, x2, col = ifelse(y_hat, "blue", "red"), xlab = "X1", ylab = "X2")
install.packages("sqldf")
